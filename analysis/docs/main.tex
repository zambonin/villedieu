\documentclass[12pt]{article}

\usepackage[T1]{fontenc}
\usepackage[a4paper, margin=1.5cm]{geometry}
\usepackage[colorlinks, urlcolor=blue, citecolor=red]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts, booktabs, enumitem, parskip, siunitx}

\newcommand{\euler}{\mathrm{e}}

\begin{document}

\textsc{Graduate Program in Computer Science,
  Universidade Federal de Santa Catarina} \\
\textsc{INE410104 (Design and Analysis of Algorithms)}

\textsc{Solutions to the 1\textsuperscript{st} Set of Exercises} \\
\textsc{Gustavo Zambonin, Matheus S. P. Bittencourt}

\section{Answers}

\begin{enumerate}
    \item 
    \begin{enumerate}
        \item Let the functions that represent the time complexities of algorithms $A$ and $B$ be $f(n) = n^{3}$ and $g(n) = 128n^{2}$, respectively, with $n \in \mathbb{N}$. Further, consider $t(n) = \frac{f(n)}{g(n)} \Rightarrow \frac{n}{128}$. By the ratio $t(n)$, algorithm $B$ is faster than $A$ when $n > 128$.
        \item Let the old and new computers be named $N_{1}$ and $N_{2}$, respectively, and $f(n) = 2^{n}$. The equation $t = \frac{2^{N_{1}}}{s}$ gives the time spent to solve an instance of $f(n)$ such that $N_{1}$ executes $s$ instructions per second. Analogously, let $t = \frac{2^{N_{2}}}{20s}$ represent the previous equation for the new computer. Solving for $N_{2}$ in terms of $N_{1}$:
        \begin{align*}
            \frac{2^{N_{1}}}{s} = \frac{2^{N_{2}}}{20s} \\
            2^{N_{2}} = 20 \cdot 2^{N_{1}} \\
            N_{2} = N_{1} + \log_{2} 20 \\
            N_{2} \approx N_{1} + 4.32.
        \end{align*}
        Ergo, one cannot execute $f(2n)$ and expect comparable results on $N_{2}$, since $N_{2} \ll 2 N_{1}$.
        
        % \item Let $t_1 = \frac{2^n}{s}$ be the running time for the old computer and $t_2 = \frac{2^{2n}}{20s}$ for the new one, with $s$ being the number of instructions that the old computer executes per minute. Let's compare the running times with the new computer solving an instance of the problem twice as large as the one being executed by the old one.
        % \begin{align*}
        %     r &= \frac{t_1}{t_2} \\
        %     &= \frac{2^n}{s}\frac{20s}{2^{2n}} \\
        %     &= \frac{20}{2^n}
        % \end{align*}
        % $r$ actually represents the speed up of the new computer, with a larger instance of the problem. The new computer will run slower with $n > 4$. This is due to the fact that the complexity of the problem grows exponentially while the computational power grew linearly.
        
    \end{enumerate}
    \item 
    \begin{enumerate}
        \item Let $t_{1}, \dots, t_{7}$ be the number of seconds in each period of the header row and $s = 10^{10}$. The values in the table below represent approximately $n \leq f^{-1}(t_{i} s)$.
        \begin{table}[htbp]
            \renewcommand{\arraystretch}{1.2}
            \setlength{\tabcolsep}{7pt}
            \centering
            \tiny
            \begin{tabular}{l*{7}{r}}
                \toprule
                $f(n)$ / $t$ & 1 second & 1 minute & 1 hour & 1 day & 1 month & 1 year & 1 century \\ \midrule
                $\log_{2} n$ & $2^{\num{1.E+10}}$ & $2^{\num{6.E+11}}$ & $2^{\num{3.6E+13}}$ & $2^{\num{8.64E+14}}$ & $2^{\num{2.592E+16}}$ & $2^{\num{3.11E+17}}$ & $2^{\num{3.11E+19}}$ \\
                $\sqrt{n}$ & $\num{1.E+20}$ & $\num{3.6E+23}$ & $\num{1.296E+27}$ & $\num{7.465E+29}$ & $\num{6.718E+32}$ & $\num{9.675E+34}$ & $\num{9.675E+38}$ \\
                $n$ & $\num{1.E+10}$ & $\num{6.E+11}$ & $\num{3.6E+13}$ & $\num{8.64E+14}$ & $\num{2.592E+16}$ & $\num{3.11E+17}$ & $\num{3.11E+19}$ \\
                $n \log_{2} n$ & $\num{3.522E+08}$ & $\num{1.763E+10}$ & $\num{9.063E+11}$ & $\num{1.957E+13}$ & $\num{5.299E+14}$ & $\num{5.936E+15}$ & $\num{5.283E+17}$ \\
                $n^{2}$ & $\num{1.E+05}$ & $\num{7.746E+05}$ & $\num{6.E+06}$ & $\num{2.939E+07}$ & $\num{1.61E+08}$ & $\num{5.577E+08}$ & $\num{5.577E+09}$ \\
                $n^{3}$ & $\num{2.154E+03}$ & $\num{8.434E+03}$ & $\num{3.302E+04}$ & $\num{9.524E+04}$ & $\num{2.959E+05}$ & $\num{6.775E+05}$ & $\num{3.145E+06}$ \\
                $2^{n}$ & $\num{3.322E+01}$ & $\num{3.913E+01}$ & $\num{4.503E+01}$ & $\num{4.962E+01}$ & $\num{5.452E+01}$ & $\num{5.811E+01}$ & $\num{6.475E+01}$ \\
                $n!$ & $\num{1.3E+01}$ & $\num{1.5E+01}$ & $\num{1.6E+01}$ & $\num{1.7E+01}$ & $\num{1.8E+01}$ & $\num{1.9E+01}$ & $\num{2.1E+01}$ \\
                \bottomrule
            \end{tabular}
        \end{table}
        \item The ``find-min'' operation is fastest on the binary and Fibonacci heaps, since it is constant. The ``delete-min'' operation should be fastest on the Fibonacci heap, since an $\mathcal{O}(\log n)$ time complexity represents an amortised asymptotic upper bound, whereas the other heaps' $\Theta(\log n)$ is tight. Indeed, given a worst case scenario, both operations take exactly the same time to complete, but otherwise the Fibonacci heap does not take $\log n$ steps to finish the operation in average. The ``insert'' operation is fastest on the binomial and Fibonacci heaps. Thus, the Fibonacci heap is overall the fastest data structure when compared to binary and binomial heaps.
        \item
        \begin{enumerate}
            \item $(\ln n)^{\ln n} \in \Omega(\frac{n}{\ln n})$, for $n_{0} > 7$ and $c = 1$.
            \item $n^{2} \in \Omega(n^{\frac{1}{2}})$, for $n_{0} > 1$ and $c = 1$.
            \item $n! \in \Omega(n^{n - 47})$, for $n_{0} > 1$ and $c = 1$.
            \item $2^{{(\ln n)}^{2}} \in \Omega((\ln n)^{\ln n})$, for $n_{0} > 1$ and $c = 1$.
            \item $n^{2} \in \Theta(n + n^{2})$, for $n_{0} > 1, c_{1} = \frac{1}{2}$ and $c_{2} = 1$.
            \item $n + (\ln n)^{2} \in \Theta(10n + \ln n)$, for $n_{0} > \frac{1}{2}, c_{1} = \frac{1}{2}$ and $c_{2} = 1$.
            \item $\ln 2n \in \Theta(\ln 3n)$, for $n_{0} > 1, c_{1} = \frac{1}{2}$ and $c_{2} = 1$.
            \item $n (\ln n)^{2} \in \mathcal{O}(\frac{n^{2}}{\ln n})$, for $n_{0} > 7$ and $c = 1$.
        \end{enumerate}
    \end{enumerate}
    \item
    \begin{enumerate}
        \item Let $P(n) = \sum_{i = 1}^{n} i = \frac{n (n + 1)}{2}$. $P(1)$ holds, since $\frac{1 (1 + 1)}{2} = 1$. The inductive step is proved below, using the hypothesis that $P(k)$ is true.
        \begin{align*}
            P(k) + (k + 1) &\stackrel{?}{=} P(k + 1) \\
            \frac{k (k + 1)}{2} + (k + 1) &\stackrel{?}{=} \\
            \frac{k (k + 1) + 2(k + 1)}{2} &\stackrel{?}{=} \\
            \frac{(k + 1)(k + 2)}{2} &\stackrel{?}{=} \\
            \frac{(k + 1)((k + 1) + 1)}{2} &= P(k + 1).
        \end{align*}
        \item Let $P(n) = \sum_{i = 0}^{n} i^{2} = \frac{n (n + 1) (2n + 1)}{6}$. $P(0)$ holds, since $\frac{0 (0 + 1) (2 \cdot 0 + 1)}{2} = 0$. The inductive step is proved below, using the hypothesis that $P(k)$ is true.
        \begin{align*}
            P(k) + (k + 1)^{2} &\stackrel{?}{=} P(k + 1) \\
            \frac{k (k + 1) (2k + 1)}{6} + (k + 1)^{2} &\stackrel{?}{=} \\
            \frac{k (k + 1) (2k + 1) + 6(k + 1)^{2}}{6} &\stackrel{?}{=} \\
            \frac{(k + 1)(k(2k + 1) + 6(k + 1))}{6} &\stackrel{?}{=} \\
            \frac{(k + 1)(2k^2 + 7k + 6)}{6} &\stackrel{?}{=} P(k + 1) \\
            &\stackrel{?}{=} \frac{(k + 1)((k + 1) + 1)(2(k + 1) + 1)}{6} \\
            \frac{(k + 1)(2k^2 + 7k + 6)}{6} &= \frac{(k + 1)(k + 2)(2k + 3)}{6}.
        \end{align*}
        \item Let $P(n) = \sum_{i = 1}^{n} (2i - 1) = n^{2}$. $P(1)$ holds, since $2 \cdot 1 - 1 = 1^{2}$. The inductive step is proved below, using the hypothesis that $P(k)$ is true.
        \begin{align*}
            P(k) + 2(k + 1) - 1 &\stackrel{?}{=} P(k + 1) \\
            k^{2} + 2(k + 1) - 1 &\stackrel{?}{=} \\
            k^{2} + 2k + 1 &\stackrel{?}{=} \\
            (k + 1)^{2} &= P(k + 1).
        \end{align*}
        \item Let $P(n) = \sum_{i = 0}^{n} i^{3} = \frac{n^{2} (n + 1)^{2}}{4}$. $P(0)$ holds, since $\frac{0^{2} \cdot (0 + 0)^{2}}{4} = 0$. The inductive step is proved below, using the hypothesis that $P(k)$ is true.
        \begin{align*}
            P(k) + (k + 1)^{3} &\stackrel{?}{=} P(k + 1) \\
            \frac{k^{2} (k + 1)^{2}}{4} + (k + 1)^{3} &\stackrel{?}{=} \\
            \frac{k^{2} (k + 1)^{2} + 4(k + 1)^{3}}{4} &\stackrel{?}{=} \\
            \frac{(k^{2} + 4(k + 1))(k + 1)^{2}}{4} &\stackrel{?}{=} \\
            \frac{(k^{2} + 4k + 4)(k + 1)^{2}}{4} &\stackrel{?}{=} \\
            \frac{(k + 2)^{2}(k + 1)^{2}}{4} &= P(k + 1).
        \end{align*}
    \end{enumerate}
    \item
    \begin{enumerate}
        \item Let \[
            T(n) = \begin{cases}
                0 &\text{if } n = 1, \\
                T(n - 1) + c &\text{if } n > 1
            \end{cases}
        \] for any constant $c$. By the iteration method,
        \begin{align*}
            T(n) &= c + T(n - 1) \\
            &= c + c + T(n - 2) \\
            &= c + c + c + T(n - 3) \cdots \\
            &= ic + T(n - i).
        \end{align*}
        Consider $T(n - i) \Leftrightarrow T(1)$. Thus, $n - i = 1$ and $i = n - 1$, which gives
        \begin{align*}
            T(n) &= (n - 1)c + T(n - (n - 1)) \\
            &= nc - c + T(1) \\
            T(n) &\in \Theta(n).
        \end{align*}
        To prove the final formula by weak induction, let $T(n) = c(n - 1)$. $T(1)$ holds, since $c(1 - 1) = 0$. The inductive step is proved below, using the hypothesis that $T(n - 1)$ is true.
        \begin{align*}
            T(n - 1) + c &\stackrel{?}{=} T(n) \\
            c((n - 1) - 1) + c &\stackrel{?}{=} \\
            c(n - 2) + c(1) &\stackrel{?}{=} \\
            c(n - 1) &= T(n).
        \end{align*}
        \item Let \[
            T(n) = \begin{cases}
                0 &\text{if } n = 1, \\
                T(n - 1) + 2^{n} &\text{if } n > 1.
            \end{cases}
        \] By the iteration method,
        \begin{align*}
            T(n) &= 2^{n} + T(n - 1) \\
            &= 2^{n} + 2^{n - 1} + T(n - 2) \\
            &= 2^{n} + 2^{n - 1} + 2^{n - 2} + T(n - 3) \cdots \\
            &= \sum_{i = 2}^{n} 2^{i}.
        \end{align*}
        The sum of this geometric series is exactly $\frac{2^{n + 1} - 1}{2 - 1} - 2^{0} - 2^{1} = 2^{n + 1} - 4$, since $i$ does not start at $0$. To prove the final formula by weak induction, let $T(n) = 2^{n + 1} - 4$. $T(1)$ holds, since $2^{1 + 1} - 4 = 0$. The inductive step is proved below, using the hypothesis that $T(n - 1)$ is true.
        \begin{align*}
            T(n - 1) + 2^{n} &\stackrel{?}{=} T(n) \\
            2^{(n - 1) + 1} - 4 + 2^{n} &\stackrel{?}{=} \\
            2^{n + 1} - 4 &= T(n).
        \end{align*}
        \item Let \[
            T(n) = \begin{cases}
                k &\text{if } n = 0, \\
                cT(n - 1) &\text{if } n > 0,
            \end{cases}
        \] for any constants $c, k$. By the iteration method,
        \begin{align*}
            T(n) &= cT(n - 1) \\
            &= c(cT(n - 2)) \\
            &= c(c(cT(n - 3))) \cdots \\
            &= c^{i} T(n - i).
        \end{align*}
        Consider $T(n - i) \Leftrightarrow T(0)$. Thus, $n - i = 0$ and $i = n$, which gives
        \begin{align*}
            T(n) &= c^{n} T(n - n)  \\
            &= c^{n} T(0) \\
            T(n) &\in \Theta(c^n).
        \end{align*}
        \item Let \[
            T(n) = \begin{cases}
                1 &\text{if } n = 1, \\
                3T(\frac{n}{2}) + n &\text{if } n > 1.
            \end{cases}
        \] By the iteration method,
        \begin{align*}
            T(n) &= n + 3T(\tfrac{n}{2}) \\
            &= n + 3(\tfrac{n}{2} + 3T(\tfrac{n}{4})) \\
            &= n + 3(\tfrac{n}{2} + 3(\tfrac{n}{4} + 3T(\tfrac{n}{8}))) \cdots \\
            &= n(\tfrac{3}{2})^{i} T(\tfrac{n}{2^{i}}).
        \end{align*}
        Consider $T(\frac{n}{2^{i}}) \Leftrightarrow T(1)$. Thus, $\frac{n}{2^{i}} = 1$ and $i = \log_{2} n$, which gives
        \begin{align*}
            T(n) &= n(\tfrac{3}{2})^{\log_{2} n} T(\tfrac{n}{2^{\log_{2} n}}) \\
            &= n(\tfrac{3}{2})^{\log_{2} n} T(1) \\
            &= nn^{\log_{2} \tfrac{3}{2}} \\
            &= nn^{\log_{2} 3 - 1} \\
            &= n^{\log_{2} 3 - 1 + 1} \\
            &= n^{\log_{2} 3} \\
            T(n) &\in \Theta(\text{poly}(n)).
        \end{align*}
        To prove the final formula by weak induction, let $T(n) = n^{\log_{2} 3}$. $P(1)$ holds, since $1^{\log_{2} 3} = 1$. The inductive step is proved below, using the hypothesis that $T(\frac{n}{2})$ is true.
        \begin{align*}
            3T(\tfrac{n}{2}) + n &\stackrel{?}{=} T(n) \\
            3(\tfrac{n}{2})^{\log_{2} 3} + n &\stackrel{?}{=} \\
            3 \cdot 2^{-\log_2 3} \cdot n^{\log_2 3} + n &\stackrel{?}{=}
        \end{align*}
    \end{enumerate}
\end{enumerate}

\appendix

\section{Rationale for Q2a}

Define $t_{1}, \dots, t_{7}$ and $s = 10^{10}$ as above. The equation $t_{i} = \frac{f(n)}{s}$, or its simpler form $n = f^{-1}(t_{i} s)$, has to be solved in order to find out the maximum $n$ allowed to execute for a period of time using $f(n)$. Deducing $f^{-1}$ is not trivial for some of the rows. In the case of $n \log_{2} n = c$ for some constant $c$, the Lambert $W$-function is used:
\begin{align*}
    n \log_{2} n &= c \\
    n \ln n &= c \ln 2 \\
    \euler^{\ln n} \ln n &= c \ln 2 \\
    \ln n &= W(c \ln 2) \\
    n &= \euler^{W(c \ln 2)}.
\end{align*}
Moreover, to invert the factorial function, some definitions from~\cite{Cantrell:200110:misc} are used to construct a strict inverse if the input is an integer. Consider the single positive zero of the digamma function $\psi_{0} \approx 1.46163214496836$ and the gamma function $\Gamma(x)$. The inverse factorial function is equal to
\begin{align*}
    \iota(n) = \left[\frac{\ln\frac{n + c}{\sqrt{2 \pi}} - 1}{W(\frac{1}{e}\ln\frac{n + c}{\sqrt{2 \pi}} - 1)} + \frac{1}{2}\right] - 1, \qquad c = \frac{1}{e} \sqrt{2 \pi} - \Gamma(\psi_{0}).
\end{align*}
The table below shows the exact values for all $n = f^{-1}(t_{i} s)$.
\begin{table}[htbp]
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{7pt}
    \centering
    \begin{tabular}{l*{7}{r}}
        \toprule
            $f(n)$ / $t$ & 1 second & 1 minute & 1 hour & 1 day & 1 month & 1 year & 1 century \\ \midrule
            $\log_{2} n$ & $2^{t_{1} s}$ & $2^{t_{2} s}$ & $2^{t_{3} s}$ & $2^{t_{4} s}$ & $2^{t_{5} s}$ & $2^{t_{6} s}$ & $2^{t_{7} s}$ \\
            $\sqrt{n}$ & $(t_{1} s)^{2}$ & $(t_{2} s)^{2}$ & $(t_{3} s)^{2}$ & $(t_{4} s)^{2}$ & $(t_{5} s)^{2}$ & $(t_{6} s)^{2}$ & $(t_{7} s)^{2}$ \\
            $n$ & $t_{1} s$ & $t_{2} s$ & $t_{3} s$ & $t_{4} s$ & $t_{5} s$ & $t_{6} s$ & $t_{7} s$ \\
            $n \log_{2} n$ & $\euler^{W(t_{1} s \ln 2)}$ & $\euler^{W(t_{2} s \ln 2)}$ & $\euler^{W(t_{3} s \ln 2)}$ & $\euler^{W(t_{4} s \ln 2)}$ & $\euler^{W(t_{5} s \ln 2)}$ & $\euler^{W(t_{6} s \ln 2)}$ & $\euler^{W(t_{7} s \ln 2)}$ \\
            $n^{2}$ & $\sqrt{t_{1} s}$ & $\sqrt{t_{2} s}$ & $\sqrt{t_{3} s}$ & $\sqrt{t_{4} s}$ & $\sqrt{t_{5} s}$ & $\sqrt{t_{6} s}$ & $\sqrt{t_{7} s}$ \\
            $n^{3}$ & $\sqrt[3]{t_{1} s}$ & $\sqrt[3]{t_{2} s}$ & $\sqrt[3]{t_{3} s}$ & $\sqrt[3]{t_{4} s}$ & $\sqrt[3]{t_{5} s}$ & $\sqrt[3]{t_{6} s}$ & $\sqrt[3]{t_{7} s}$ \\
            $2^{n}$ & $\log_{2}(t_{1} s)$ & $\log_{2}(t_{2} s)$ & $\log_{2}(t_{3} s)$ & $\log_{2}(t_{4} s)$ & $\log_{2}(t_{5} s)$ & $\log_{2}(t_{6} s)$ & $\log_{2}(t_{7} s)$ \\
            $n!$ & $\iota(t_{1} s)$ & $\iota(t_{2} s)$ & $\iota(t_{3} s)$ & $\iota(t_{4} s)$ & $\iota(t_{5} s)$ & $\iota(t_{6} s)$ & $\iota(t_{7} s)$ \\
        \bottomrule
    \end{tabular}
\end{table}

\bibliographystyle{alpha}
{\footnotesize
\bibliography{ref}}

\end{document}
